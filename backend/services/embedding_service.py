"""
Embedding Service - G√©n√©ration d'embeddings avec FastEmbed

‚ö†Ô∏è IMPORTANT: Utilise FastEmbed uniquement (pas de Hugging Face)
FastEmbed est optimis√© pour Qdrant et offre de meilleures performances.
"""

from fastembed import TextEmbedding
from typing import List
import numpy as np


class EmbeddingService:
    """Service pour g√©n√©rer des embeddings s√©mantiques avec FastEmbed"""
    
    def __init__(self, model_name: str = "BAAI/bge-small-en-v1.5"):
        """
        Initialise FastEmbed avec un mod√®le optimis√©
        
        Mod√®les disponibles:
        - BAAI/bge-small-en-v1.5 (384 dim) - Rapide et efficace (D√âFAUT)
        - sentence-transformers/all-MiniLM-L6-v2 (384 dim)
        - BAAI/bge-base-en-v1.5 (768 dim) - Plus pr√©cis
        
        Args:
            model_name: Nom du mod√®le FastEmbed √† utiliser
        """
        print(f"üß† Initialisation FastEmbed: {model_name}")
        self.model = TextEmbedding(model_name=model_name)
        self.model_name = model_name
        
        # D√©terminer la dimension du mod√®le
        test_embedding = list(self.model.embed(["test"]))[0]
        self.dimension = len(test_embedding)
        print(f"‚úÖ FastEmbed pr√™t - Dimension: {self.dimension}D")
        print(f"‚ö†Ô∏è AUDIT: FastEmbed utilis√© (pas de Hugging Face/SentenceTransformers)")
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        G√©n√®re un embedding pour un texte unique
        
        Args:
            text: Texte √† encoder
            
        Returns:
            Vecteur d'embedding (liste de floats)
        """
        embeddings = list(self.model.embed([text]))
        return embeddings[0].tolist()
    
    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        G√©n√®re des embeddings pour plusieurs textes (batch)
        Plus efficace que des appels individuels
        
        Args:
            texts: Liste de textes √† encoder
            
        Returns:
            Liste de vecteurs d'embeddings
        """
        embeddings = list(self.model.embed(texts))
        return [emb.tolist() for emb in embeddings]
    
    def create_product_text(self, name: str, description: str, category: str = "") -> str:
        """
        Cr√©e un texte optimis√© pour l'embedding d'un produit
        
        Format: "category | name | description"
        
        Args:
            name: Nom du produit
            description: Description du produit
            category: Cat√©gorie du produit (optionnel)
            
        Returns:
            Texte format√© pour l'embedding
        """
        parts = []
        if category:
            parts.append(category)
        parts.append(name)
        if description:
            parts.append(description)
        return " | ".join(parts)
    
    def get_dimension(self) -> int:
        """Retourne la dimension des embeddings"""
        return self.dimension
    
    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        Calcule la similarit√© cosinus entre deux vecteurs
        
        Args:
            vec1: Premier vecteur
            vec2: Deuxi√®me vecteur
            
        Returns:
            Score de similarit√© (0.0 √† 1.0)
        """
        vec1_np = np.array(vec1)
        vec2_np = np.array(vec2)
        
        dot_product = np.dot(vec1_np, vec2_np)
        norm1 = np.linalg.norm(vec1_np)
        norm2 = np.linalg.norm(vec2_np)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
